\documentclass{beamer}
\usetheme{Warsaw}
\usepackage{hyperref}
\usepackage{amsmath}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Parsing Natural Scenes and Natural Language with Recursive Neural Networks - ICML11}
\author{Seungwoo Yoo}
\date{\today}

\begin{document}
\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Introduction}
\subsection{Introduction to RNN}
\frame
{
  \frametitle{Parsing Natural Structural Examples}
  \begin{columns}
  \column{0.5\textwidth}
  \begin{figure}[ht]  
	  \begin{center}
		  \includegraphics[width=2.1in]{images/fig1.png}   
	  \end{center}   
  \end{figure}
  \column{0.5\textwidth} 
  \begin{figure}[ht]
	  \begin{center}
		  \includegraphics[width=2.1in]{images/fig2.png} 
	  \end{center}
  \end{figure}
  \end{columns}
  \begin{itemize}
  \item Ilustration of recursive neural network architectures
	\begin{enumerate}
	\item Segment features and word indices are mapped into semantic feature space
	\item Recursively merged by the same NN until they represent the entire image or sentence
	\end{enumerate}
  \end{itemize}
}
\frame
{
	\frametitle{Contributions}
	\begin{itemize}
		\item RNN can compute (for scene parsing):
			\begin{enumerate}
			\item a score that is higher when neighboring regions should be merged into a larger region
			\item a new semantic feature representation for this larger region and its class labels (e.g., \textit{building} or \textit{street})
			\end{enumerate}
		\item Contribution points
			\begin{itemize}
			\item First deep learning method to achieve state-of-the-art results on segmentation and annotations of complex scenes. (ICML11)
			\item For scene classification, learned features outperform state of the art methos such as Gist descriptor 
			\item Code for the RNN model is available at \url{http://www.socher.org}
			\end{itemize}
	\end{itemize}
}
\section{Algorithm Details}
\frame{\tableofcontents}
\subsection{Mapping Segments and Words into Syntactic-Semantic Space}
\frame
{
	\frametitle{Input Representation of Scene Images}
	\begin{itemize}
		\item Feature extraction
			\begin{enumerate}
				\item Oversegment an image \textit{x} into superpixels using Meanshift algo. (only choose one set of parameters)
					\begin{itemize}
					\item This results in an average of 78 segments per image. 
					\end{itemize}
				\item Compute 119 dim features for the segments (same as \textit{Gould ICCV 2009} - color, texture, appearance, shape  features and boosted pixel classifier scores).
				\item Use a simple neural network layer to map these features into "semantic" \textit{n}-dim feature space :
				$$ a_i = f(W^{\textit{sem}}F_i+b^{sem}) $$ 
				$$ s.t., W^{sem} \in \mathbb{R}^{n\times119} \;and\; f(x)=1/(1+e^{-x})$$
			\end{enumerate}
	\end{itemize}
}
\frame
{
	\frametitle{Input Representation of Natural Language Sentences}
	\begin{itemize}
		\item Use a neural language models
		$$ a_i = Le_k \in \mathbb{R}^n \; s.t. \; L \in \mathbb{R}^{n\times|V|} $$
		\item Neural language model maps words to a vector representation, which stored in a word embedding matrix \textit{L}
		\item This matrix captures co-occurrence statistics and its values are pre-learned. 
	\end{itemize}
}
\subsection{Recursive Neural Networks for Structure Prediction}
\frame
{
	\frametitle{RNN inputs for training}
	\begin{columns}
	\column{0.5\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.1in]{images/fig3.png}   
		\end{center}   
		\caption{Illustrations of the RNN training inputs}
	\end{figure}
	\column{0.6\textwidth} 
	\begin{itemize}
	\item Goal : Learn a function $ f:\textit{X}\rightarrow\textit{Y} $
	\item Input $ x $ consists of two parts:
		\begin{enumerate}
		\item A set of activation vectors (semantic features) ${a_1, \dots, a_{N_{segs}}}$
		\item A symmetric adjacency matrix $ A $
		\end{enumerate}
		\item We have labels $l$ for all segments $\rightarrow$ can define a set of correct trees $Y(x, l)$ 
	\end{itemize}
	\end{columns}
}
\frame
{
	\frametitle{Max-Margin Estimation}
	\begin{itemize}
		\item Define a structured margin loss $\Delta(x,l,\hat{y})$ similar to \textit{Taskar et. al., 2004}
		\begin{Definition}
		$$ \Delta(x,l, \hat{y}) = \kappa \sum_{d \in \textbf{N}(\hat{y})} \textbf{1}\{\textit{subTree}(d) \notin Y(x,l)\}$$
		where $ \textbf{N}(\hat{y}) $ is the set of non-terminal nodes
		\end{Definition}
		\item The loss increases when a segment merges with another one of a different label before merging with all its neighbors of the same label. 
		\item Formulated by checking whether the sub-tree $\textit{subTree}(d)$ underneath a nonterminal node $d$ 
		in $\hat{y}$ appears in any of the ground truth trees of $Y(x,l)$
	\end{itemize}
}
\frame
{
	\begin{itemize}
		\item Search for a function $ f $ with small expected loss on unseen inputs:
		$$ f_{\theta}(x) = \argmax_{\hat{y}\in\tau(x)} s(\text{RNN}(\theta, x, \hat{y}))$$
		where $\theta$ are all the parameters to compute a score $ s $ with an RNN and $ \tau(x) $
		denotes the set of all possible trees that can be constructed from an input x. 
		\item Margin constraints (score of the highest scoring correct tree $y_i$ 
		to be larger up to a margin defined by the loss $ \Delta $)
		$$ s(\text{RNN}(\theta, x_i, y_i)) \geq s(\text{RNN}(\theta, x_i, \hat{y})) + \Delta(x_i, l_i, \hat{y})$$
		where $\forall i, \hat{y} \in \tau(x_i)$
	\end{itemize}
}
\frame
{
	\begin{itemize}
		\item Regularized risk function :
	\end{itemize}
		$$ J(\theta) = \frac{1}{N}\sum_{i=1}^N r_i(\theta) + \frac{\lambda}{2}||\theta||^2 \; where$$
		$$ r_i(\theta) = \max_{\hat{y}\in\tau(x_i)} (s(\text{RNN}(\theta, x, \hat{y}) + \Delta(x_i, l_i, \hat{y})) - 
		\max_{y_i \in Y_{(x_i, l_i)}} (s\text{RNN}(\theta, x_i, y_i)) $$

	\begin{itemize}
		\item Minimize this objective $\rightarrow$ maximizes the correct tree's score and minimizes (up to a margin) 
		the score of the highest scoring 
	\end{itemize}
}
\frame
{
	\frametitle{Greedy Structure Prediction of RNNs}
	\begin{itemize}
		\item Hard to find an optimal structure 
		\begin{itemize}
		\item No efficient algorithms for RNN setting and $\exists$ exponentially 
		many possible parse trees $\rightarrow$ greedy approximation
		\end{itemize}
		\item Greedy approximation 
		\begin{itemize}
		\item Find the pairs of neighboring segments and adds their semantic features to a set of potential child node pairs:
		$$ C = \{[a_i, a_j]: A(i,j) = 1\} $$
		\end{itemize}
	\end{itemize}
}
\frame
{
	\begin{columns}
	\column{0.5\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.1in]{images/fig3.png}   
		\end{center}   
	\end{figure}
	\column{0.6\textwidth} 
	\begin{itemize}
		\item For example :
		$$ C = \{[a_1, a_2], [a_1, a_3], [a_2, a_1], $$ 
		$$ [a_2, a_4], \dots  [a_4,a_5], [a_5, a_4]\} $$
	\end{itemize}
	\end{columns}
}
\frame
{
	\begin{columns}
	\column{0.4\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.25in]{images/fig4.png}   
		\end{center}   
	\end{figure}
	\column{0.6\textwidth} 
	\begin{itemize}
		\item The network computes the potential 
		parent representation in recursive way for the possible child nodes as $ \textit{p} $
		\item Local score can be calculated using a row vector 
		$ W^{score} \in \mathbb{R}^{1\times n} $
	\end{itemize}
	\end{columns}
}
\frame
{
	\begin{itemize}
		\item After computing the scores for all pairs of neighboring segments, 
		the algorithm selects the pair received the highest score. 
		\item Let the score $s_{ij}$ be the highest score
		\begin{enumerate}
			\item Remove $[a_i, a_j] $ from $C$ as well as all other pairs with 
			either $a_i$ or $a_j$ in them. 
			\item Update the adjacency matrix reflecting the new segment having the neighbors of both child segments
			\item Add potential new child pairs to $C$:
			$$ C = C - \{[a_i,a_j]\} - \{[a_j, a_i]\} $$
			$ C = C \cup \{[p_{(i,j)}, a_k]: a_k $ has boundary with $i$ or $j\}$
		\end{enumerate}
	\end{itemize}
}
\frame
{
	\begin{columns}
	\column{0.45\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.in]{images/fig3.png}   
		\end{center}   
	\end{figure}
	\column{0.73\textwidth} 
	\begin{itemize}
		\item For example, if we merge $[a_4, a_5]$ then 
		$$ C = \{[a_1, a_2], [a_1, a_3], [a_2, a_1], [a_2, p_{(4,5)}], $$
		$$ [a_3, a_1], [a_3, p_{(4,5)}], [p_{(4,5)}, a_2], [p_{(4,5)}, a_3]\} $$
	\end{itemize}
	\end{columns}
}
\frame
{
	\begin{itemize}
		\item The merging process repeats until all pairs are merged
		\begin{itemize}
		\item The same network (with parameters $W, b, W^{score}$) is \emph{recursively} applied
		\item Tree can be recovered by unfolding the decision steps
		\item Final score for structure prediction (sum of all the local decision) :
		$$ s(\text{RNN}(\theta, x, \hat{y})) = \sum_{d\in N(\hat{y})} s_d  $$ 
		$$ s_d = W^{score}_d p $$ 
		$$ p = f(W[c_i;c_j] + b) $$
		\end{itemize}
	\end{itemize}
}
\frame
{
	\begin{columns}
	\column{0.45\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.in]{images/fig3.png}   
		\end{center}   
	\end{figure}
	\column{0.73\textwidth} 
	\begin{itemize}
		\item Assume the next highest score was $s_{((4,5),3)}$
		$$ C = \{[a_1, a_2], [a_1, p_{((4,5), 3)}], [a_2, a_1], $$
		$$ [a_2, p_{((4,5), 3)}], [p_{((4,5), 3)}, a_1], [p_{((4,5), 3)}, a_2]\} $$
		\item Finally, merge segment $(1,2) \rightarrow$ right bottom tree. 
	\end{itemize}
	\end{columns}
}
\frame
{
	\frametitle{Category Classifiers in the Tree}
	\begin{itemize}
		\item Can add a softmax layer to each RNN parent node (after removing scoring layer) to predict class labels:
		$$ label_p = softmax(W^{label}p) $$ 
		\item Minimizing the cross-entropy error $\rightarrow$ errors can be backpropagated.
	\end{itemize}
}
\subsection{Learning}
\frame
{
	\frametitle{Minimize the loss}
	\begin{itemize}
		\item Objective $J$ is not differentiabl due to hinge loss $\rightarrow$
		generate gradient descent via the subgradient method for structure learning
		\begin{itemize}
			\item Model parameter $\theta = (W^{sem}, W, W^{score}, W^{label}), $ then gradient becomes : 
			$$ \frac{\partial J}{\partial \theta} = \frac{1}{N} \sum_i \frac{\partial s(\hat{y}_i)}{\partial \theta} 
			- \frac{\partial s(y_i)}{\partial \theta} + 	\lambda \theta $$ where $s(\hat{y}_i) = s(\text{RNN}(\theta, x_i, \hat{y}_{max (\tau(x_i))})) $ and
			$ s(y_i) = s(\text{RNN}(\theta, x_i, y_{max(Y(x_i, l_i))})) $
			\item To calculate gradients, use backpropagation through structure (BPTS - Goller \& K$\ddot{\text{u}}$chler, 1996)
		\end{itemize}		
	\end{itemize}
}
\frame
{
	\frametitle{Backpropagation through Structure (BPTS)}
	\begin{itemize}
		\item Principally same as general backpropagation
		\item Error messages are splitted at each node and then propagated to the childeren
		\item Two differences resulting from the tree structure : 
		\begin{itemize}
			\item Split derivatives at each node
			\item Sum derivatives of $W$ from all nodes
		\end{itemize}
		\item For optimization, plug the gradients in to the L-BFGS optimizaer (use batch training)
		\item Best result were obtained using AdaGrad(Duchi et.al, 2011)
	\end{itemize}
}
\section{Experimental Results}
\frame{\tableofcontents}
\subsection{Scene Understanding: Segmentation and Annotation}
\frame
{
  \frametitle{Parsing Natural Scene Images}
  \begin{figure}[ht]  
  	\begin{center}
  		\includegraphics[width=3.1in]{images/ex5.png}   
		\caption{Pixel level multi-class segmentation accuracies. Stanford background datset is used and run five-fold crossvalidation. (Simply label the superpixels by their most likely class based on RNN)}
	\end{center}
  \end{figure}
}
\frame
{
  \begin{figure}[ht]  
  	\begin{center}
  		\includegraphics[height=2.3in]{images/ex2.png}   
		\caption{Nearest neighbor image region tree - visualize neareset neighbor super segment using Euclidean distance between the vector representations.}
	\end{center}
  \end{figure}
}
\frame
{
  \begin{figure}[ht]  
  	\begin{center}
  		\includegraphics[height=3.in]{images/ex1.png}   
	\end{center}
  \end{figure}
}
\frame
{
  \begin{columns}
  \column{0.5\textwidth}
  \begin{figure}[ht]  
	  \begin{center}
		  \includegraphics[width=2.1in]{images/ex1_ext.png}   
	  \end{center}   
  \end{figure}
  \column{0.5\textwidth} 
  \begin{figure}[ht]
	  \begin{center}
		  \includegraphics[width=2.1in]{images/ex1_ext2.png} 
	  \end{center}
  \end{figure}
  \end{columns}
}
\subsection{Text Parsing and Nearest Neighbor Phrases}
\frame
{
  \frametitle{Supervised Parsing}
  \begin{itemize}
  	\item Train models using Wall Street Journal section of the Penn Treebank
  	\item Performance of language parser (F-measure) is 90.29\% compared to 91.63\% for the widely used Berkeley parser (Petrov et.al., 2006)
  	\item 72 seconds to parse 421 sentences (in the Matlab implementation)
  \end{itemize}
}
\frame
{
	\frametitle{Nearest Neighbor Phrases}
	\begin{columns}
	\column{0.6\textwidth}
	\begin{figure}[ht]  
		\begin{center}
			\includegraphics[width=2.5in]{images/ex4.png}   
		\end{center}   
	\end{figure}
	\column{0.5\textwidth} 
	\begin{itemize}
		\item Collect nearest neighbors for nodes of multi-word phrases
		\item Embed complete sentences from WSJ dataset into the semantic space
	\end{itemize}
	\end{columns}
}
\section{Discussion}
\frame
{
	\frametitle{Discussion - Simple RNN approach}
	\begin{itemize}
		\item Good results with single matrix RNN (efficient)
		\item Single weight matrix RNN could capture some phenonmena but not adquate for more complete, higher order composition and parsing problems.
		\item Cannot correctly predict phrase or word labels that are determined by context (in NLP): 
			\begin{itemize}
				\item Cannot predict that the word \textit{Android} is positive in the sentence \textit{Android beats iOS}
			\end{itemize}
	\end{itemize}
}
\end{document}
